---
layout: ../layouts/Layout.astro
title: "GelFusion: Enhancing Robotic Manipulation under Visual Constraints via Visuotactile Fusion"
description: Enhancing Robotic Manipulation under Visual Constraints via Visuotactile Fusion
favicon: Gel.png
thumbnail: screenshot-light.png
---

import Header from "../components/Header.astro";
import Video from "../components/Video.astro";
import HighlightedSection from "../components/HighlightedSection.astro";
import SmallCaps from "../components/SmallCaps.astro";
import Figure from "../components/Figure.astro";
import Image from "../components/Image.astro";
import TwoColumns from "../components/TwoColumns.astro";
import YouTubeVideo from "../components/YouTubeVideo.astro";
import LaTeX from "../components/LaTeX.astro";
import Notes from "../components/Notes.astro";

import { ImageComparison } from "../components/ImageComparison.tsx";

import Framework from "../assets/Framework.png"
import Cross_attention from "../assets/Cross_attention.png"
import sensor from "../assets/sensor.png"
import InsertationLongDemo from "../assets/Insertation.mp4"
import comming_soon from "../assets/comming.png"
import midlle_wiping from "../assets/middle_wiping.mp4"
import up_wiping from "../assets/up_wiping.mp4"
import down_wiping from "../assets/down_wiping.mp4"
import chipSuccess from "../assets/chipSuccess.mp4"
import chipFail from "../assets/chipFail.mp4"
import Diff_Shapes1 from "../assets/Diff_Shapes1.mp4"


import CodeBlock from "../components/CodeBlock.astro";
import Table from "../components/Table.astro";
export const components = {pre: CodeBlock, table: Table}

<Header
  title={frontmatter.title}
  authors={[
    {
      name: "Shulong Jinag",
      notes: ["1"],
    },
    {
      name: "Shiqi Zhao",
      notes: ["1"],
    },
    {
      name: "Yuxuan Fan",
      notes: ["2"],
    },
    {
      name: "Peng Yin",
      notes: ["1"],
    },
  ]}
  notes={[
    {
      symbol: "1",
      text: "City University of Hong Kong",
    },
    {
      symbol: "2",
      text: "The Hong Kong University of Science and Technology (Guangzhou)",
    },
  ]}
  links={[
    {
      name: "Paper",
      url: "https://arxiv.org/pdf/2505.07455v1",
      icon: "ri:file-pdf-2-line",
    },
    {
      name: "Code",
      url: "",
      icon: "ri:github-line",
    },
    {
      name: "arXiv",
      url: "https://arxiv.org/abs/2505.07455v1",
      icon: "academicons:arxiv",
    }
  ]}
  />


<Figure>
<YouTubeVideo slot="figure" videoId="YwoxCyPOEAY" />
</Figure>

## Abstract

Visuotactile sensing offers rich contact information that can help mitigate performance bottlenecks in imitation learning, particularly under vision-limited conditions, such as ambiguous visual cues or occlusions. Effectively fusing visual and visuotactile modalities, however, presents ongoing challenges. 
We introduce GelFusion, a framework designed to enhance policies by integrating visuotactile feedback, specifically from high-resolution GelSight sensors. GelFusion using a vision-dominated cross-attention fusion mechanism incorporates visuotactile information into policy learning. To better provide rich contact information, the framework's core component is our dual-channel visuotactile feature representation, simultaneously leveraging both texture-geometric and dynamic interaction features. 
We evaluated GelFusion on three contact-rich tasks: surface wiping, peg insertion, and fragile object pick-and-place. Outperforming baselines, GelFusion shows the value of its structure in improving the success rate of policy learning.

<hr style="border: none; height: 3px; background-color: #000; margin: 20px 0;"/>

## Visuotactile Sensor - Gelsight

GelSight, a classic high-resolution visuotactile sensor, captures membrane deformation with an internal camera to encode tactile information. provides rich contact information detailing **surface properties** and **interaction dynamics**.

<Figure>
  <Image slot="figure" source={sensor} altText="sensor" />
  <span slot="caption">Gelsight Sensor.</span>
</Figure>

## Network

To address the fusion challenge, we introduce GelFusion, a **vision-led** framework ,that integrates visuotactile feedback and vision inputs for policy learning, employing a **cross-attention mechanism** for vision-dominant cross-modal fusion.

<Figure>
  <Image slot="figure" source={Framework} altText="Framework" />
  <span slot="caption">Network of GelFusion.</span>
</Figure>

The Cross-attention module fuses visual and tactile information by calculating weights based on the **visual query** to dynamically emphasize the relevance of each modality's features. These weighted features are then combined to form a fused representation that guides the diffusion policy.

<Figure>
  <Image slot="figure" source={Cross_attention} altText="Cross_attention" />
  <span slot="caption">Cross-attention module.</span>
</Figure>

## Task Evaluatio

### Wiping Task
The experiment evaluated GelFusion's assistance to robots in surface wiping tasks, specifically addressing vision's limitations such as **depth ambiguity** in establishing precise contact and maintaining consistent pressure and orientation for effective cleaning. It comprised two critical contact phases: initially establishing contact from an arbitrary height, followed by maintaining consistent pressure and orientation during the wiping action.

>To better handle the writing on the whiteboard in the observed images, we adjusted camera parameters such as exposure to deal with the issue of light reflection.

### Typical Failure Case
<TwoColumns>
  <Figure slot="left">  
    <Image slot="figure" source={comming_soon} altText="Framework" />
    <span slot="caption">Float Wiping.</span> 
  </Figure>
  <Figure slot="right">
    <Image slot="figure" source={comming_soon} altText="Framework" />
    <span slot="caption">Overpress.</span>
  </Figure>
</TwoColumns>

### Different Test Scenarios
<TwoColumns>
  <Figure slot="left">  
    <Video slot="figure" source={Diff_Shapes1} altText="Framework" />
    <span slot="caption">Different Line Shapes.</span> 
  </Figure>
  <Figure slot="right">
    <Video slot="figure" source={up_wiping} altText="Framework" />
    <span slot="caption">Different Line Pisitions.</span>
  </Figure> 
</TwoColumns>

<TwoColumns>
  <Figure slot="left">  
    <Image slot="figure" source={comming_soon} altText="Framework" />
    <span slot="caption">Different Line Shapes.</span> 
  </Figure>
  <Figure slot="right">
    <Video slot="figure" source={midlle_wiping} altText="Framework" />
    <span slot="caption">Different Line Pisitions.</span>
  </Figure> 
</TwoColumns>

<TwoColumns>
  <Figure slot="left">  
    <Image slot="figure" source={comming_soon} altText="Framework" />
    <span slot="caption">Different Line Shapes.</span> 
  </Figure>
  <Figure slot="right">
    <Video slot="figure" source={down_wiping} altText="Framework" />
    <span slot="caption">Different Line Pisitions.</span>
  </Figure> 
</TwoColumns>


{/* =============================================================================================== */}


### Insertation Task
The peg insertion task, designed to evaluate precise alignment capabilities, primarily challenges robots by simulating vision-limited scenarios, particularly during the final insertion phase where **visual occlusion is inherent**. This task, comprising critical phases including grasping, transport, alignment, insertion, and release, was evaluated under significant randomization of peg and puzzle box positions and orientations, with distractor holes present, to assess the policy's robustness and reliance on tactile feedback for success.

<Figure>
<Video slot="figure" source={InsertationLongDemo} altText="Framework" />
<span slot="caption">Insertation Long-Term Test.</span>
</Figure>

### Typical Failure Case
<TwoColumns>
  <Figure slot="left">  
    <Image slot="figure" source={comming_soon} altText="Framework" />
    <span slot="caption">Bad alignment.</span> 
  </Figure>
  <Figure slot="right">
    <Image slot="figure" source={comming_soon} altText="Framework" />
    <span slot="caption">Transport Collision.</span>
  </Figure> 
</TwoColumns>

{/* =============================================================================================== */}



### Chips Pick Task
The task objective is robotic grasping of highly fragile objects like potato chips. As brittle materials prone to sudden fracture under excessive force, **force control** is crucial for maintaining theirstructural integrity. This task highlights the limitations of relying solely on wrist-camera vision, which lacks the necessary force-sensitive feedback to prevent such instantaneous damage.

### Demonstration

<TwoColumns>
  <Figure slot="left">  
    <Video slot="figure" source={chipFail} altText="Framework" />
    <span slot="caption">Failure Case.</span> 
  </Figure>
  <Figure slot="right">
    <Video slot="figure" source={chipSuccess} altText="Framework" />
    <span slot="caption">Success Case.</span>
  </Figure> 
</TwoColumns>

Vision-only policy(Diffusion Policy) often fail, not by crushing the chips, but by **applying too little force due to premature grasp timing decisions based on similar visual inputs**. GelFusion, with its tactile feedback, ensures accurate force control, enabling successful and delicate handling.

---

## BibTeX citation

```bibtex
@misc{jiang2025gelfusionenhancingroboticmanipulation,
      title={GelFusion: Enhancing Robotic Manipulation under Visual Constraints via Visuotactile Fusion}, 
      author={Shulong Jiang and Shiqi Zhao and Yuxuan Fan and Peng Yin},
      year={2025},
      eprint={2505.07455},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2505.07455}, 
}
```