---
layout: ../layouts/Layout.astro
title: GelFusion:Enhancing Robotic Manipulation under Visual Constraints via Visuotactile Fusion
description: Enhancing Robotic Manipulation under Visual Constraints via Visuotactile Fusion
favicon: Gel.png
thumbnail: screenshot-light.png
---

import Header from "../components/Header.astro";
import Video from "../components/Video.astro";
import HighlightedSection from "../components/HighlightedSection.astro";
import SmallCaps from "../components/SmallCaps.astro";
import Figure from "../components/Figure.astro";
import Image from "../components/Image.astro";
import TwoColumns from "../components/TwoColumns.astro";
import YouTubeVideo from "../components/YouTubeVideo.astro";
import LaTeX from "../components/LaTeX.astro";

import { ImageComparison } from "../components/ImageComparison.tsx";

import outside from "../assets/outside.mp4";
import transformer from "../assets/transformer.webp";
import Splat from "../components/Splat.tsx"
import dogsDiffc from "../assets/dogs-diffc.png"
import dogsTrue from "../assets/dogs-true.png"

import Framework from "../assets/Framework.png"
import Cross_attention from "../assets/Cross_attention.png"
import sensor from "../assets/sensor.png"


import CodeBlock from "../components/CodeBlock.astro";
import Table from "../components/Table.astro";
export const components = {pre: CodeBlock, table: Table}

<Header
  title={frontmatter.title}
  authors={[
    {
      name: "Shulong Jinag",
      notes: ["1"],
    },
    {
      name: "Shiqi Zhao",
      notes: ["1"],
    },
    {
      name: "Yuxuan Fan",
      notes: ["2"],
    },
    {
      name: "Peng Yin",
      notes: ["1"],
    },
  ]}
  notes={[
    {
      symbol: "1",
      text: "City University of Hong Kong",
    },
    {
      symbol: "2",
      text: "The Hong Kong University of Science and Technology (Guangzhou)",
    },
  ]}
  links={[
    {
      name: "Paper",
      url: "",
      icon: "ri:file-pdf-2-line",
    },
    {
      name: "Code",
      url: "",
      icon: "ri:github-line",
    },
    {
      name: "arXiv",
      url: "",
      icon: "academicons:arxiv",
    }
  ]}
  />

{/*<Video source={outside}>*/}



## Abstract

Visuotactile sensing offers rich contact information that can help mitigate performance bottlenecks in imitation learning, particularly under vision-limited conditions, such as ambiguous visual cues or occlusions. Effectively fusing visual and visuotactile modalities, however, presents ongoing challenges. 
We introduce GelFusion, a framework designed to enhance policies by integrating visuotactile feedback, specifically from high-resolution GelSight sensors. GelFusion using a vision-dominated cross-attention fusion mechanism incorporates visuotactile information into policy learning. To better provide rich contact information, the framework's core component is our dual-channel visuotactile feature representation, simultaneously leveraging both texture-geometric and dynamic interaction features. 
We evaluated GelFusion on three contact-rich tasks: surface wiping, peg insertion, and fragile object pick-and-place. Outperforming baselines, GelFusion shows the value of its structure in improving the success rate of policy learning.

<hr style="border: none; height: 3px; background-color: #000; margin: 20px 0;"/>

## Visuotactile sensor - Gelsight

GelSight, a classic high-resolution visuotactile sensor, captures membrane deformation with an internal camera to encode tactile information. provides rich contact information detailing **surface properties** and **interaction dynamics**.

<Figure>
  <Image slot="figure" source={sensor} altText="sensor" />
  <span slot="caption">Gelsight Sensor.</span>
</Figure>

## Network

To address the fusion challenge, we introduce GelFusion, a **vision-led** framework ,that integrates visuotactile feedback and vision inputs for policy learning, employing a **cross-attention mechanism** for vision-dominant cross-modal fusion.

<Figure>
  <Image slot="figure" source={Framework} altText="Framework" />
  <span slot="caption">Network of GelFusion.</span>
</Figure>

The Cross-attention module fuses visual and tactile information by calculating weights based on the **visual query** to dynamically emphasize the relevance of each modality's features. These weighted features are then combined to form a fused representation that guides the diffusion policy.

<Figure>
  <Image slot="figure" source={Cross_attention} altText="Cross_attention" />
  <span slot="caption">Cross-attention module.</span>
</Figure>



## BibTeX citation

```bibtex
@misc{roman2024academic,
  author = "{Roman Hauksson}",
  title = "Academic Project Page Template",
  year = "2024",
  howpublished = "\url{https://research-template.roman.technology}",
}
```